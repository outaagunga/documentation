
Perfect üëç Let‚Äôs build a **ready-to-use template** for your assignment.
I‚Äôll structure it in two parts for each sector:

1. **Report template (Word/PDF style)**
2. **Code/Excel snippets you can adapt**

---

# üìù Report Template (Per Sector)

## Sector: \[Insert Sector Name]

### 1. Initial Problems Identified

* \[List missing values, duplicates, typos, wrong formats, etc.]

### 2. Cleaning Techniques Applied

* \[Step 1: e.g., Removed duplicates using Excel "Remove Duplicates".]
* \[Step 2: Standardized date format to DD-MM-YYYY using Pandas `to_datetime`.]
* \[Step 3: Filled missing prices using median values.]
* \[Step 4: ‚Ä¶]

### 3. Challenges Faced

* \[E.g., Some product names had multiple spellings, required manual mapping.]
* \[E.g., GPS coordinates had invalid entries that needed validation.]

### 4. Tools Used & Justification

* **Excel/Google Sheets** ‚Üí quick visual cleaning, easy to show screenshots.
* **Python (Pandas)** ‚Üí reproducible, efficient for bulk data cleaning.

### 5. Screenshots / Code Snippets

*(Paste here screenshots of Excel steps or short Python code blocks)*

---

# üêç Python Snippet Template (Per Dataset)

```python
import pandas as pd

# Load dataset
df = pd.read_csv("dataset.csv")

# 1. Remove duplicates
df = df.drop_duplicates()

# 2. Standardize date format
df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime("%d-%m-%Y")

# 3. Standardize text fields (example: product/medication/crop names)
df['product'] = df['product'].str.strip().str.title()

# 4. Handle missing values (example: fill median for price, drop for category)
df['price'] = df['price'].fillna(df['price'].median())
df = df.dropna(subset=['category'])

# 5. Validate numeric ranges (example: ages, loan amounts)
df = df[df['age'].between(0, 120)]
df = df[df['loan_amount'] < 5000000]

# Save cleaned dataset
df.to_csv("dataset_cleaned.csv", index=False)
```

---

# üìä Excel/Google Sheets Tips

* **Remove Duplicates** ‚Üí Data > Remove Duplicates
* **Text Cleanup** ‚Üí TRIM(), PROPER(), UPPER()
* **Date Conversion** ‚Üí Format > Number > Date (custom `DD-MM-YYYY`)
* **Missing Values** ‚Üí Use `=IF(ISBLANK(A2), "Unknown", A2)`
* **Outlier Check** ‚Üí Use filters or conditional formatting (highlight values above threshold)

---

‚ö° Next step: I can generate **4 pre-filled sector-specific reports** with sample wording + corresponding Python/Excel steps for each.

Do you want me to draft those **sector reports (Supermarket, Healthcare, FinTech, Agriculture)** so you just customize them with your actual data?



Great üöÄ Let‚Äôs draft **sector-specific reports + cleaning snippets**. You‚Äôll only need to plug in your actual dataset names and maybe tweak a few details.

---

# üõí **Sector 1: Supermarket (Retail)**

## 1. Initial Problems

* Duplicate transaction records.
* Inconsistent date formats (`01/01/25`, `2025-01-01`).
* Product names inconsistent (`sugar`, `Sugar `, `SUGR`).
* Missing product categories and prices.
* Currency formats not consistent (some in KES, others missing symbols).

## 2. Cleaning Techniques

* Removed duplicates using Excel ‚ÄúRemove Duplicates‚Äù and Pandas `.drop_duplicates()`.
* Standardized dates to `DD-MM-YYYY`.
* Standardized product names with `.str.strip().str.title()` and Excel `PROPER()`.
* Filled missing prices with median, dropped rows missing category.
* Ensured all amounts were numeric and formatted in KES.

## 3. Challenges

* Some product names required manual correction (e.g., `SUGR ‚Üí Sugar`).
* Some prices had currency text (`KES 100`) that needed stripping.

## 4. Tools Used

* **Excel**: Quick duplicate removal and missing value checks.
* **Python (Pandas)**: Automated text cleanup and formatting.

---

### üîπ Python Snippet (Supermarket)

```python
df = pd.read_csv("supermarket.csv")

df = df.drop_duplicates()
df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime("%d-%m-%Y")
df['product'] = df['product'].str.strip().str.title()
df['price'] = df['price'].replace('[^0-9.]','', regex=True).astype(float)
df['price'] = df['price'].fillna(df['price'].median())
df = df.dropna(subset=['category'])

df.to_csv("supermarket_cleaned.csv", index=False)
```

---

# üè• **Sector 2: Healthcare (Clinic)**

## 1. Initial Problems

* Missing values in `age` and `diagnosis`.
* Inconsistent medication names (`Panadol ` vs `panadol`).
* Appointment dates in different formats.
* Personal identifiers (patient names, IDs).
* Invalid ages (e.g., `300`).

## 2. Cleaning Techniques

* Filled missing ages with median; dropped rows missing diagnosis.
* Standardized medication names with `.str.strip().str.title()`.
* Converted dates to `DD-MM-YYYY`.
* Removed names/IDs for anonymization.
* Dropped rows with invalid ages (>120).

## 3. Challenges

* Some patients had no age but valid diagnosis ‚Üí chose not to drop.
* Different spellings for the same drug required manual grouping.

## 4. Tools Used

* **Excel**: Quick anonymization, filters for outliers.
* **Python (Pandas)**: Automating date/age cleaning.

---

### üîπ Python Snippet (Healthcare)

```python
df = pd.read_csv("healthcare.csv")

df['age'] = df['age'].apply(lambda x: None if x > 120 else x)
df['age'] = df['age'].fillna(df['age'].median())
df['medication'] = df['medication'].str.strip().str.title()
df['appointment_date'] = pd.to_datetime(df['appointment_date'], errors='coerce').dt.strftime("%d-%m-%Y")
df = df.drop(['patient_name','id_number'], axis=1)

df = df.dropna(subset=['diagnosis'])
df.to_csv("healthcare_cleaned.csv", index=False)
```

---

# üí≥ **Sector 3: FinTech (Mobile Loans)**

## 1. Initial Problems

* Incorrect mobile numbers (missing digits, inconsistent prefixes).
* Missing repayment status.
* Inconsistent customer locations (`Nrb`, `Nairobi`).
* Loan amounts/interest as text instead of numbers.
* Outlier loan amounts (e.g., 50M for microloan).

## 2. Cleaning Techniques

* Normalized mobile numbers (forced `+2547XXXXXXXX`).
* Filled missing repayment status with ‚ÄúUnknown‚Äù.
* Standardized names/locations.
* Converted amounts to numeric.
* Removed outliers above 5,000,000.

## 3. Challenges

* Many phone numbers had leading zeros or `+254` inconsistencies.
* Hard to distinguish outliers vs. genuine high loans.

## 4. Tools Used

* **Excel**: Filters for phone numbers, repayment status.
* **Python (Pandas)**: Bulk numeric cleaning, outlier removal.

---

### üîπ Python Snippet (FinTech)

```python
df = pd.read_csv("fintech.csv")

df['mobile'] = df['mobile'].astype(str).str.replace(r'\D','', regex=True)
df['mobile'] = df['mobile'].apply(lambda x: '+254' + x[-9:] if len(x) >= 9 else None)

df['repayment_status'] = df['repayment_status'].fillna("Unknown")
df['location'] = df['location'].str.strip().str.title().replace({"Nrb":"Nairobi"})

df['loan_amount'] = pd.to_numeric(df['loan_amount'], errors='coerce')
df = df[df['loan_amount'] < 5000000]

df.to_csv("fintech_cleaned.csv", index=False)
```

---

# üåΩ **Sector 4: Agriculture (AgriTech)**

## 1. Initial Problems

* Duplicate farmer entries.
* Inconsistent crop names (`Maize`, `maize `, `MAIZE`).
* Missing values in quantity/price.
* Invalid GPS coordinates.
* Misspelled district names.

## 2. Cleaning Techniques

* Removed duplicate farmer IDs.
* Standardized crop names with `.str.strip().str.title()`.
* Filled missing prices with median, dropped rows missing quantity.
* Validated GPS format (lat, long).
* Replaced district names with standard Nairobi sub-counties list.

## 3. Challenges

* Some GPS values had extra characters (`1.234, xyz`).
* District name misspellings needed manual dictionary mapping.

## 4. Tools Used

* **Excel**: Quick duplicates removal and GPS inspection.
* **Python (Pandas)**: Crop name standardization, numeric cleaning.

---

### üîπ Python Snippet (Agriculture)

```python
df = pd.read_csv("agriculture.csv")

df = df.drop_duplicates(subset=['farmer_id'])
df['crop'] = df['crop'].str.strip().str.title()
df['price'] = df['price'].fillna(df['price'].median())
df = df.dropna(subset=['quantity'])

# Validate GPS (basic check for commas & numeric values)
df = df[df['gps'].str.contains(',', na=False)]
df = df[df['gps'].str.match(r'^-?\d+\.\d+,\s*-?\d+\.\d+$')]

# District corrections
district_map = {"Westlnds":"Westlands", "Embksi":"Embakasi"}
df['district'] = df['district'].replace(district_map)

df.to_csv("agriculture_cleaned.csv", index=False)
```

---

‚úÖ With this, you have **4 ready-to-use reports** and **Python/Excel snippets**.

Do you want me to also make a **combined master template file** (Word/Docx + sample CSVs) that you can directly edit and submit?
